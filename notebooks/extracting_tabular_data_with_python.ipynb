{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting tabular data from OCR-processed PDFs with Python\n",
    "\n",
    "This notebook explores a method of extracting tabular data from OCR-processed PDF files using Python. We will use two key software libraries: [poppler](https://poppler.freedesktop.org/) and [pdftabextract](https://pypi.python.org/pypi/pdftabextract/). Poppler will be used to extract the images from the PDF files and pdftabextract will be used to extract the tabular data. We will then anaylse the successfulness of this method, how feasible it is to perform on a large scale and what might be done to improve it.\n",
    "\n",
    "The dataset that we will use contains samples of digitised material from the [India Office Medical Archives](https://www.bl.uk/collection-guides/india-office-medical-archive-collections). The dataset was created by Antonia Moon (2017) and is available for [download](https://data.bl.uk/indiaofficemedicalarchives/ioma3.html) via data.bl.uk. The printed material in this dataset contains tabular data on medical topography, which we will attempt to extract for further analysis. An example of a digitised image from the collection is presented below.\n",
    "\n",
    "![An example of a digitised image from the collection](https://data.bl.uk/images/ioma2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial assumes that you already have [Python](https://www.python.org/) installed and have some familiarity with running Python scripts.\n",
    "\n",
    "With that in mind, we will start by installing the required software libraries via [PyPi](https://pypi.python.org/pypi). Open up a command-line interface and run the code below. There is no need to include the exclamation mark at the start of the code block, which is used to install the libraries within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests tqdm pdftabextract numpy pandas opencv-python --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "We can now start writing our Python script. \n",
    "\n",
    "Using a text editor, create a new file and save it as `run.py`, then enter the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import tqdm\n",
    "import zipfile\n",
    "import cv2\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pdftabextract\n",
    "from pdftabextract import imgproc\n",
    "from pdftabextract.geom import pt\n",
    "from pdftabextract.common import read_xml, parse_pages, all_a_in_b, save_page_grids\n",
    "from pdftabextract.common import DIRECTION_VERTICAL, ROTATION, SKEW_X, SKEW_Y\n",
    "from pdftabextract.textboxes import rotate_textboxes, deskew_textboxes\n",
    "from pdftabextract.textboxes import border_positions_from_texts, split_texts_by_positions, join_texts\n",
    "from pdftabextract.clustering import calc_cluster_centers_1d, find_clusters_1d_break_dist, zip_clusters_and_values\n",
    "from pdftabextract.extract import make_grid_from_positions, fit_texts_into_grid, datatable_to_dataframe\n",
    "\n",
    "from math import radians, degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare some common variables\n",
    "\n",
    "There are some common variables that will be used in various places throughout the tutorial. We will declare these below the imports for easier reference. The comments above each variable indicate their purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory to which we will download our dataset.\n",
    "DATA_DIR = '../data'\n",
    "\n",
    "# An HTTP header that we add to identify our application over a network.\n",
    "USER_AGENT = 'bl-digischol-notebooks'\n",
    "\n",
    "# The name of the dataset collection\n",
    "COLLECTION = 'indiaofficemedicalarchives'\n",
    "\n",
    "# The name of the dataset\n",
    "DATASET = 'ioma-samples-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "We now need to download our dataset and extract the files is contains. The code block below will handle this programmatically. Copy the code into your Python script, save the file, then open up a command-line interface, navigate to the location of your script and run the following:\n",
    "\n",
    "```\n",
    "python run.py\n",
    "```\n",
    "\n",
    "Assuming the dataset does not already exist in the correct location it will be downloaded and the files extracted. For more details about how the process works, see [Downloading datasets with Python](downloading_datasets_with_python.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    \n",
    "\n",
    "def download_dataset(collection, dataset, directory, user_agent):\n",
    "    url = 'https://data.bl.uk/{0}/{1}.zip'.format(collection, dataset)\n",
    "    download_fn = url.split('/')[-1]\n",
    "    download_path = os.path.join(directory, download_fn)\n",
    "    if not os.path.exists(download_path):\n",
    "        headers = {'User-agent': user_agent}\n",
    "        r = requests.get(url, stream=True, headers=headers)\n",
    "        total_length = int(r.headers.get('Content-Length'))\n",
    "        total_size = (total_length/1024) + 1\n",
    "        with open(download_path, 'wb') as f:\n",
    "            for chunk in tqdm.tqdm(r.iter_content(chunk_size=1024), \n",
    "                                   total=total_size, \n",
    "                                   desc='Downloading', \n",
    "                                   unit='kb',\n",
    "                                   unit_scale=True, \n",
    "                                   miniters=1, \n",
    "                                   leave=False): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "\n",
    "def extract_dataset(dataset, collection, data_dir):\n",
    "    fn = '{}.zip'.format(dataset)\n",
    "    in_path = os.path.join(data_dir, fn)\n",
    "    out_path = os.path.join(data_dir, collection)\n",
    "    with zipfile.ZipFile(in_path) as archive:\n",
    "        unextracted = [name for name in archive.namelist() \n",
    "                       if not os.path.exists(os.path.join(out_path, name))]\n",
    "        if unextracted:\n",
    "            for i in tqdm.tqdm(range(len(unextracted)), \n",
    "                               desc='Extracting', \n",
    "                               unit='file', \n",
    "                               leave=False):\n",
    "                archive.extract(unextracted[i], path=out_path)\n",
    "\n",
    "\n",
    "create_data_dir(DATA_DIR)\n",
    "download_dataset(COLLECTION, DATASET, DATA_DIR, USER_AGENT)\n",
    "extract_dataset(DATASET, COLLECTION, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the images from the PDF files\n",
    "\n",
    "As a first step to accessing OCR data embedded in the image files we need to extract the images from our PDF files and convert the OCR data to XML. To do this we use a PDF rendering library called [poppler](https://github.com/davidben/poppler), which is available in most Linux distributions. \n",
    "\n",
    "Note that if running this notebook on a Windows machine, the latest binaries for poppler can be downloaded [here](http://blog.alivate.com.au/poppler-windows/). For Mac, install [Homebrew](https://brew.sh/), open a terminal and run `brew install poppler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(data_dir, collection):\n",
    "    base_dir = os.path.join(data_dir, collection)\n",
    "    pdfs = [fn for fn in os.listdir(base_dir) if fn.endswith('.pdf')] \n",
    "    xmls = ['{}.xml'.format(os.path.splitext(pdf)[-2]) for pdf in pdfs]\n",
    "    unconverted = [fn for fn in xmls if not os.path.exists(os.path.join(base_dir, fn))]\n",
    "\n",
    "    if unconverted:\n",
    "        for fn in tqdm.tqdm(unconverted, desc='Converting', unit='file', leave=False):\n",
    "            pdf_path = os.path.join(base_dir, '{}.pdf'.format(os.path.splitext(fn)[-2]))\n",
    "            xml_path = os.path.join(base_dir, fn)\n",
    "            script = 'C:/users/amendes/downloads/poppler-0.51/bin/pdftohtml -c -q -hidden -xml \"{0}\" \"{1}\"'.format(pdf_path, xml_path)\n",
    "            !{script}\n",
    "\n",
    "extract_images(DATA_DIR, COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have an XML file containing our OCR data, along with a JPEG image for each page of the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect lines in the images\n",
    "\n",
    "The following code is largely taken from the online tutorial [Data Mining OCR PDFs — Using pdftabextract to Liberate Tabular Data from Scanned Documents](https://datascience.blog.wzb.eu/2017/02/16/data-mining-ocr-pdfs-using-pdftabextract-to-liberate-tabular-data-from-scanned-documents/) (Markus Konrad, 2017), which includes a detailed explanation about the programatic process below. \n",
    "\n",
    "Essentially, we identify the lines in each image, attempt to fix any rotation or skewing, cluster similar lines, identify any grids and output the data contanied within those grids to a set of tables. The result should be the conversion of data from printed to electronic tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to save an image \n",
    "def save_image_w_lines(iproc_obj, imgfilebasename):\n",
    "    img_lines = iproc_obj.draw_lines(orig_img_as_background=True)\n",
    "    img_lines_file = os.path.join('%s-lines-orig.png' % imgfilebasename)\n",
    "    cv2.imwrite(img_lines_file, img_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lines(iproc_obj, img_base_fn, metadata):\n",
    "\n",
    "    # calculate the scaling of the image file in relation to the text boxes\n",
    "    page_scaling_x = iproc_obj.img_w / metadata['width']\n",
    "    page_scaling_y = iproc_obj.img_h / metadata['height']\n",
    "\n",
    "    # detect the lines\n",
    "    lines_hough = iproc_obj.detect_lines(canny_kernel_size=3, canny_low_thresh=50, canny_high_thresh=150,\n",
    "                                         hough_rho_res=1,\n",
    "                                         hough_theta_res=np.pi/500,\n",
    "                                         hough_votes_thresh=int(round(0.2 * iproc_obj.img_w)))\n",
    "\n",
    "    # save the image with detected lines\n",
    "    save_image_w_lines(iproc_obj, img_base_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix rotation or skew\n",
    "\n",
    "If the tables are rotated or skewed we'll have a hard time extracting the data from them. The following code fixes this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_rotation_and_skew(iproc_obj, img_base_fn, metadata, xml, xmltree):\n",
    "    rot_or_skew_type, rot_or_skew_radians = iproc_obj.find_rotation_or_skew(radians(0.5),\n",
    "                                                                            radians(1),\n",
    "                                                                            omit_on_rot_thresh=radians(0.5))\n",
    "    # rotate back or deskew text boxes\n",
    "    needs_fix = True\n",
    "    if rot_or_skew_type == ROTATION:\n",
    "        rotate_textboxes(metadata, -rot_or_skew_radians, pt(0, 0))\n",
    "    elif rot_or_skew_type in (SKEW_X, SKEW_Y):\n",
    "        deskew_textboxes(metadata, -rot_or_skew_radians, rot_or_skew_type, pt(0, 0))\n",
    "    else:\n",
    "        needs_fix = False\n",
    "\n",
    "    if needs_fix:\n",
    "        # rotate back or deskew detected lines\n",
    "        lines_hough = iproc_obj.apply_found_rotation_or_skew(rot_or_skew_type, \n",
    "                                                             -rot_or_skew_radians)\n",
    "\n",
    "        save_image_w_lines(iproc_obj, img_base_fn + '-repaired')\n",
    "\n",
    "    out_base_fn = xml[:xml.rindex('.')]\n",
    "    repaired_xmlfile = os.path.join(out_base_fn + '.repaired.xml')\n",
    "    xmltree.write(repaired_xmlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_lines(iproc_obj, img_base_fn, metadata):\n",
    "    MIN_COL_WIDTH = 60 # minimum width of a column in pixels\n",
    "\n",
    "    # cluster the detected *vertical* lines using find_clusters_1d_break_dist as simple clustering function\n",
    "    page_scaling_x = iproc_obj.img_w / metadata['width']\n",
    "    vertical_clusters = iproc_obj.find_clusters(imgproc.DIRECTION_VERTICAL, find_clusters_1d_break_dist,\n",
    "                                                remove_empty_cluster_sections_use_texts=metadata['texts'],\n",
    "                                                remove_empty_cluster_sections_n_texts_ratio=0.1,   \n",
    "                                                remove_empty_cluster_sections_scaling=page_scaling_x,\n",
    "                                                dist_thresh=MIN_COL_WIDTH/2)\n",
    "\n",
    "    # draw the clusters\n",
    "    try:\n",
    "        img_w_clusters = iproc_obj.draw_line_clusters(imgproc.DIRECTION_VERTICAL, vertical_clusters)\n",
    "    except Exception as e:\n",
    "        print('ERROR:' + e.message)\n",
    "        return\n",
    "    \n",
    "    save_img_file = os.path.join('%s-vertical-clusters.png' % img_base_fn)\n",
    "    cv2.imwrite(save_img_file, img_w_clusters)\n",
    "    return vertical_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_row_positions(iproc_obj, img_base_fn, metadata, vertical_clusters):\n",
    "    page_scaling_x = iproc_obj.img_w / metadata['width']\n",
    "    page_colpos = np.array(calc_cluster_centers_1d(vertical_clusters)) / page_scaling_x\n",
    "    \n",
    "    # right border of the second column\n",
    "    col2_rightborder = page_colpos[2]\n",
    "\n",
    "    # calculate median text box height\n",
    "    median_text_height = np.median([t['height'] for t in metadata['texts']])\n",
    "\n",
    "    # get all texts in the first two columns with a \"usual\" textbox height\n",
    "    # we will only use these text boxes in order to determine the line positions because they are more \"stable\"\n",
    "    # otherwise, especially the right side of the column header can lead to problems detecting the first table row\n",
    "    text_height_deviation_thresh = median_text_height / 2\n",
    "    texts_cols_1_2 = [t for t in metadata['texts']\n",
    "                      if t['right'] <= col2_rightborder\n",
    "                         and abs(t['height'] - median_text_height) <= text_height_deviation_thresh]\n",
    "    \n",
    "    # get all textboxes' top and bottom border positions\n",
    "    borders_y = border_positions_from_texts(texts_cols_1_2, DIRECTION_VERTICAL)\n",
    "\n",
    "    # break into clusters using half of the median text height as break distance\n",
    "    clusters_y = find_clusters_1d_break_dist(borders_y, dist_thresh=median_text_height/2)\n",
    "    clusters_w_vals = zip_clusters_and_values(clusters_y, borders_y)\n",
    "\n",
    "    # for each cluster, calculate the median as center\n",
    "    pos_y = calc_cluster_centers_1d(clusters_w_vals)\n",
    "    pos_y.append(metadata['height'])\n",
    "\n",
    "    print('number of line positions:', len(pos_y))\n",
    "\n",
    "    # a (possibly malformed) population number + space + start of city name\n",
    "    pttrn_table_row_beginning = re.compile(r'^[\\d Oo][\\d Oo]{2,} +[A-ZÄÖÜ]')\n",
    "\n",
    "    # 1. try to find the top row of the table\n",
    "    texts_cols_1_2_per_line = split_texts_by_positions(texts_cols_1_2, pos_y, DIRECTION_VERTICAL,\n",
    "                                                       alignment='middle',\n",
    "                                                       enrich_with_positions=True)\n",
    "\n",
    "    # go through the texts line per line\n",
    "    for line_texts, (line_top, line_bottom) in texts_cols_1_2_per_line:\n",
    "        line_str = join_texts(line_texts)\n",
    "        if pttrn_table_row_beginning.match(line_str):  # check if the line content matches the given pattern\n",
    "            top_y = line_top\n",
    "            break\n",
    "    else:\n",
    "        top_y = 0\n",
    "    \n",
    "    # hints for a footer text box\n",
    "    words_in_footer = ('anzeige', 'annahme', 'ala')\n",
    "\n",
    "    # 2. try to find the bottom row of the table\n",
    "    min_footer_text_height = median_text_height * 1.5\n",
    "    min_footer_y_pos = metadata['height'] * 0.7\n",
    "    # get all texts in the lower 30% of the page that have are at least 50% bigger than the median textbox height\n",
    "    bottom_texts = [t for t in metadata['texts']\n",
    "                    if t['top'] >= min_footer_y_pos and t['height'] >= min_footer_text_height]\n",
    "    bottom_texts_per_line = split_texts_by_positions(bottom_texts,\n",
    "                                                     pos_y + [metadata['height']],   # always down to the end of the page\n",
    "                                                     DIRECTION_VERTICAL,\n",
    "                                                     alignment='middle',\n",
    "                                                     enrich_with_positions=True)\n",
    "    # go through the texts at the bottom line per line\n",
    "    page_span = page_colpos[-1] - page_colpos[0]\n",
    "    min_footer_text_width = page_span * 0.8\n",
    "    for line_texts, (line_top, line_bottom) in bottom_texts_per_line:\n",
    "        line_str = join_texts(line_texts)\n",
    "        has_wide_footer_text = any(t['width'] >= min_footer_text_width for t in line_texts)\n",
    "        # check if there's at least one wide text or if all of the required words for a footer match\n",
    "        if has_wide_footer_text or all_a_in_b(words_in_footer, line_str):\n",
    "            bottom_y = line_top\n",
    "            break\n",
    "    else:\n",
    "        bottom_y = metadata['height']\n",
    "    \n",
    "    page_rowpos = [y for y in pos_y if top_y <= y <= bottom_y]\n",
    "    return page_colpos, page_rowpos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(page_colpos, page_rowpos, page_num, out_base_fn, output_dir):\n",
    "    grid = make_grid_from_positions(page_colpos, page_rowpos)\n",
    "    if not grid:\n",
    "        return\n",
    "    \n",
    "    n_rows = len(grid)\n",
    "    n_cols = len(grid[0])\n",
    "    \n",
    "    page_grids_file = os.path.join(output_dir, out_base_fn + '.pagegrids_p3_only.json')\n",
    "    save_page_grids({page_num: grid}, page_grids_file)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 0.00page [00:00, ?page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of line positions:', 37)\n",
      "                   col1             col2 col3\n",
      "0     M.  D. ,  Offic i   ati n g  Civil     \n",
      "1                                            \n",
      "2                                            \n",
      "3  sc h unde r  Du tt .                      \n",
      "4                                            \n",
      "5       E  NAT I VE  ME         DI C A L     \n",
      "6                                            \n",
      "7       CER'S  REPOR T.                      \n",
      "8                                            \n",
      "9        h s '  Ret urn  of  the  Pati e  nts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def extract_tables(data_dir, collection):\n",
    "    base_dir = os.path.join(data_dir, collection)\n",
    "    xmls = [fn for fn in os.listdir(base_dir) if os.path.splitext(fn)[-1] == '.xml']\n",
    "    data = []\n",
    "    \n",
    "    for xml in xmls:\n",
    "        xml_path = os.path.join(base_dir, xml)\n",
    "        xmltree, xmlroot = read_xml(xml_path)\n",
    "        out_base_fn = xml[:xml.rindex('.')]\n",
    "        pages = parse_pages(xmlroot)\n",
    "        \n",
    "        for page_num, page in tqdm.tqdm(enumerate(pages),\n",
    "                                        desc='Processing',\n",
    "                                        unit='page',\n",
    "                                        unit_scale=True,\n",
    "                                        leave=False):\n",
    "            if page_num != 17:\n",
    "                continue\n",
    "                \n",
    "            metadata = pages[page_num]\n",
    "            img_path = metadata['image']\n",
    "            iproc_obj = imgproc.ImageProc(img_path)\n",
    "            \n",
    "            detect_lines(iproc_obj, out_base_fn, metadata)\n",
    "            fix_rotation_and_skew(iproc_obj, out_base_fn, metadata, xml, xmltree)\n",
    "            vertical_clusters = cluster_lines(iproc_obj, out_base_fn, metadata)\n",
    "            page_colpos, page_rowpos = find_row_positions(iproc_obj, out_base_fn, metadata, vertical_clusters)\n",
    "            grid = create_grid(page_colpos, page_rowpos, page_num, out_base_fn, base_dir)\n",
    "            if grid:\n",
    "                datatable = fit_texts_into_grid(metadata['texts'], grid)\n",
    "                df = datatable_to_dataframe(datatable)\n",
    "                print(df.head(10))\n",
    "\n",
    "extract_tables(DATA_DIR, COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "Konrad, M. (2017) *Data Mining OCR PDFs — Using pdftabextract to Liberate Tabular Data from Scanned Documents*, WZB Science Blog, February 16, 2017, https://datascience.blog.wzb.eu/2017/02/16/data-mining-ocr-pdfs-using-pdftabextract-to-liberate-tabular-data-from-scanned-documents/\n",
    "\n",
    "Moon, A. (2017) *India Office Medical Archives samples (small)*. British Library. https://doi.org/10.21250/ioma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
